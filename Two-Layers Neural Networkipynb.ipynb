{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW4.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "63LW835YrFl-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Two-Layers Neural Network: Scratch/TensorFlow/Pytorch\n",
        "    This notebook shows the basic idea for two-layers neural network and how to develop it from scratch as well as use Tensorflow and Pytorch.\n",
        "\n",
        "    The architecture of the neural network is connected by two forward pass, one rectified linear unit (ReLU) and one softmax function. Forward pass combine the input feature and the hidden laery by weights and bias. ReLU is general nonlinear part within modern neural network and softmax can generate multi-class probability for the prediction."
      ]
    },
    {
      "metadata": {
        "id": "gxU6wb8Obd5W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "import torch\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RxsGPrq1vz0O",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Scratch Version\n",
        "    In scratch version,the input data is an (nxp) image and the first layer contain 512 neurons whcih require px512 weights and 512 bias. After the linear conbination, ReLU is implemented to generate nonlinear activation. The second layer contain 10 neurons inorder to generate 10 output categories for MNIST prediction. To find out which category has the highest score, a softmax function is utilized.\n",
        "    \n",
        "    After the forward pass, the cross-entrophy is calculated and the corresponding gradient with respect to weights and bias are computed. Through the iteration, weights and bias will be update until the accuracy achieve 85 percent for MNIST prediction."
      ]
    },
    {
      "metadata": {
        "id": "7auiUKDacFC9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "###############################################\n",
        "## Function 1: Write 2-layer NN from scratch ##\n",
        "###############################################\n",
        "\n",
        "def my_NN_scratch(mnist):\n",
        "\n",
        "    X_test = mnist.test.images\n",
        "    Y_test = mnist.test.labels\n",
        "    ntest = X_test.shape[0]\n",
        "    num_hidden = 512\n",
        "    num_iterations = 5000\n",
        "    learning_rate =3.5e-3\n",
        "\n",
        "    #######################\n",
        "    ## FILL IN CODE HERE ##\n",
        "    #######################\n",
        "    np.random.seed(2)\n",
        "    d = num_hidden\n",
        "    p = X_test.shape[1]\n",
        "    n = 128\n",
        "    W1 = np.random.randn(p,d)*1e-2\n",
        "    b1 = np.zeros((1,d))\n",
        "    W2 = np.random.randn(d,Y_test.shape[1])*1e-2\n",
        "    b2 = np.zeros((1,Y_test.shape[1]))\n",
        "    alpha = np.zeros((p+1,d)) #(p+1)xd\n",
        "    beta = np.zeros((d+1,Y_test.shape[1])) #(d+1)x10\n",
        "    \n",
        "    \n",
        "\n",
        "    for it in range(num_iterations):\n",
        "        \n",
        "        #Forward Proporgation\n",
        "        batch_xs, batch_ys = mnist.train.next_batch(n)\n",
        "\n",
        "        #FC1\n",
        "        h1 = np.dot(batch_xs,W1)+b1 #(n,p)(p,d)+(1,d)=(n,d)\n",
        "        #ReLU\n",
        "        S1 = np.maximum(0,h1) #(n,d)\n",
        "        #assert(S1.shape == h1.shape) #nxd\n",
        "        \n",
        "        #FC2\n",
        "        h2 =  np.dot(S1,W2)+b2 #(n,d)(d,10)+(1,10)=(n,10)\n",
        "        #Softmax\n",
        "        Pr = np.zeros((n,Y_test.shape[1])) #nx10\n",
        "        for i in range(n):\n",
        "          Max = np.max(h2[i,:])\n",
        "          Pr[i,:]=np.exp(h2[i,:]-Max)/np.sum(np.exp(h2[i,:]-Max)) #avoid overflow: subtract maximum\n",
        "        \n",
        "        #Back_Softmax\n",
        "        dh2 = (1/n)*(batch_ys-Pr)#nx10\n",
        "        \n",
        "        #BackFC\n",
        "        dW2 = np.dot(dh2.T,S1).T #(dL/dh2)(dh2/dW2) ((n,10).T(n,d)).T=(d,10)\n",
        "        db2 = np.dot(np.ones((1,n)),dh2) #(dL/dh2)(dh2/db2) (1,n)(n,10)=(1,10)\n",
        "        \n",
        "        #BackReLU\n",
        "        dS1 = np.dot(dh2,W2.T) #(dL/dh2)(dh2/dS1) ((n,10)(d,10).T)=(n,d)\n",
        "        dh1 = np.array(dS1, copy=True)\n",
        "        dh1[dS1<0]=0 #nxd\n",
        "        \n",
        "        #BackFC\n",
        "        dW1 = np.dot(dh1.T,batch_xs).T#(dL/dh1)(dh1/dW1) ((n,d).T(n,p)).T\n",
        "        db1 = np.dot(dh1.T,np.ones((n,1))).T#(dL/dh1)(dh1/db1) ((n,d).T(n,1)).T\n",
        "\n",
        "        \n",
        "        #Updated\n",
        "        W2 = W2 + learning_rate * dW2\n",
        "        b2 = b2 + learning_rate * db2\n",
        "        W1 = W1 + learning_rate * dW1\n",
        "        b1 = b1 + learning_rate * db1\n",
        "        \n",
        "        alpha[1:, :] = W1\n",
        "        alpha[0, :] = b1\n",
        "        beta[1:, :] = W2\n",
        "        beta[0, :] = b2\n",
        "        \n",
        "        #######################\n",
        "        ## FILL IN CODE HERE ##\n",
        "        #######################\n",
        "\n",
        "    return alpha[1:, :], alpha[0, :], beta[1:, :], beta[0, :]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zngG_uxiveZG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## TensorFlow Version\n",
        "    The following part utilize TensorFlow to build up the two-layers neural network. The builtin function can efficiently develop the forward pass and the backward pass. Due to the optimized methodology in TensorFLow, more hidden layers and neurons can be implemented so that the accuracy can achieve 96%."
      ]
    },
    {
      "metadata": {
        "id": "tx3mdIDhV1R7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def my_NN_tensorflow(mnist):\n",
        "\n",
        "    num_hidden = 1024\n",
        "    x = tf.placeholder(tf.float32, [None, 784])\n",
        "\n",
        "    W1 = tf.Variable(tf.random_normal([784,num_hidden])) # Define it\n",
        "    b1 = tf.Variable(tf.random_normal([num_hidden])) # Define it\n",
        "    W2 = tf.Variable(tf.random_normal([num_hidden,10])) # Define it\n",
        "    b2 = tf.Variable(tf.random_normal([10])) # Define it\n",
        "    z = tf.nn.relu(tf.matmul(x, W1) + b1)\n",
        "    y = tf.matmul(z, W2) + b2\n",
        "\n",
        "    y_ =  tf.placeholder(tf.float32, [None, 10])\n",
        "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y, labels=y_)) # Define it formula:-sum([Y*log(P)]cross_entrophy) /n\n",
        "    train_step = tf.train.GradientDescentOptimizer(0.075).minimize(cross_entropy)\n",
        "    sess = tf.InteractiveSession()\n",
        "    tf.global_variables_initializer().run()\n",
        "    for epoch in range(6000):\n",
        "        batch_xs, batch_ys = mnist.train.next_batch(100)\n",
        "        res = sess.run(train_step,feed_dict = {x: batch_xs,y_: batch_ys})  # Define it\n",
        "    \n",
        "    #Test\n",
        "    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "    print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\n",
        "    W1_e, b1_e, W2_e, b2_e = W1.eval(), b1.eval(), W2.eval(), b2.eval()\n",
        "    sess.close()\n",
        "\n",
        "    return W1_e, b1_e, W2_e, b2_e"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qLkc6S4lvi1W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Pytorch Version\n",
        "    Pythoch also has efficient builtin function that is helpful to develop forward and backward pass. The other powerful function that is utilized here is the optimizer such as stoachistic graident descent method, which increase the accuracy to 96%."
      ]
    },
    {
      "metadata": {
        "id": "CpB3GzTihZc5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "\n",
        "def my_NN_pytorch(mnist_m):\n",
        "\n",
        "    class Net(torch.nn.Module):\n",
        "      def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 =  nn.Linear(784,100)# Define it\n",
        "        self.fc2 =  nn.Linear(100,10)# Define it\n",
        "        \n",
        "      def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))# Define it\n",
        "        x = self.fc2(x)# Define it\n",
        "        return x\n",
        "\n",
        "    net = Net()\n",
        "    #net.zero_grad() ? should not be this; should be optimizer\n",
        "    Loss =  nn.CrossEntropyLoss() # Define it cross-entrophy (combine softmax and null-loss)\n",
        "    optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9) # Define it\n",
        "\n",
        "    for epoch in range(4000):  # loop over the dataset multiple times\n",
        "\n",
        "        batch_xs, batch_ys = mnist_m.train.next_batch(100)\n",
        "        #######################\n",
        "        ## FILL IN CODE HERE ##\n",
        "        #######################\n",
        "        batch_xs = torch.as_tensor(batch_xs)\n",
        "        batch_ys = torch.as_tensor(batch_ys)\n",
        "        batch_ys = Variable(batch_ys).long()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(batch_xs)\n",
        "        loss = Loss(outputs, batch_ys) # input: Torch.floatTensor; level:Torch.longTensor\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    params = list(net.parameters())\n",
        "    return params[0].detach().numpy().T, params[1].detach().numpy(), \\\n",
        "        params[2].detach().numpy().T, params[3].detach().numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FSv3jW4_vp2_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Evaluate the training"
      ]
    },
    {
      "metadata": {
        "id": "e-3NbW5cm5Li",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def evaluate(W1, b1, W2, b2, data):\n",
        "\n",
        "    inputs = data.test.images\n",
        "    outputs = np.dot(np.maximum(np.dot(inputs, W1) + b1, 0), W2) + b2\n",
        "    predicted = np.argmax(outputs, axis=1)\n",
        "    accuracy = np.sum(predicted == data.test.labels)*100 / outputs.shape[0]\n",
        "    print('Accuracy of the network on test images: %.f %%' % accuracy)\n",
        "    return accuracy\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wDoY70ohm60S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "outputId": "3522aad0-9fab-4083-a574-599ca1d43497"
      },
      "cell_type": "code",
      "source": [
        "def main_test():\n",
        "\n",
        "    mnist = input_data.read_data_sets('input_data', one_hot=True)\n",
        "    mnist_m = input_data.read_data_sets('input_data', one_hot=False)\n",
        "    W1, b1, W2, b2 = my_NN_scratch(mnist)\n",
        "    evaluate(W1, b1, W2, b2, mnist_m)\n",
        "    W1, b1, W2, b2 = my_NN_tensorflow(mnist)\n",
        "    evaluate(W1, b1, W2, b2, mnist_m)\n",
        "    W1, b1, W2, b2 = my_NN_pytorch(mnist_m)\n",
        "    evaluate(W1, b1, W2, b2, mnist_m)\n",
        "\n",
        "main_test()\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting input_data/train-images-idx3-ubyte.gz\n",
            "Extracting input_data/train-labels-idx1-ubyte.gz\n",
            "Extracting input_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting input_data/t10k-labels-idx1-ubyte.gz\n",
            "Extracting input_data/train-images-idx3-ubyte.gz\n",
            "Extracting input_data/train-labels-idx1-ubyte.gz\n",
            "Extracting input_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting input_data/t10k-labels-idx1-ubyte.gz\n",
            "Accuracy of the network on test images: 85 %\n",
            "WARNING:tensorflow:From <ipython-input-17-e7dd6f512899>:14: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "0.956\n",
            "Accuracy of the network on test images: 96 %\n",
            "Accuracy of the network on test images: 96 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}